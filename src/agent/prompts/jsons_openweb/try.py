x = """RootWebArea 'fixie-ai/ultravox-v0_4 · Hugging Face', focused, url='https://huggingface.co/fixie-ai/ultravox-v0_4'\n\t[134] banner '', visible\n\t\t[137] link "Hugging Face's logo Hugging Face", clickable, visible, url='https://huggingface.co/'\n\t\t\t[138] image "Hugging Face's logo", visible, url='https://huggingface.co/front/assets/huggingface_logo-noborder.svg'\n\t\t\tStaticText 'Hugging Face'\n\t\t[141] textbox 'Search models, datasets, users...', clickable, visible\n\t\t[146] navigation 'Main', visible\n\t\t\t[147] list '', visible\n\t\t\t\t[148] listitem '', visible\n\t\t\t\t\t[149] link 'Models', clickable, visible, url='https://huggingface.co/models'\n\t\t\t\t[151] listitem '', visible\n\t\t\t\t\t[152] link 'Datasets', clickable, visible, url='https://huggingface.co/datasets'\n\t\t\t\t[154] listitem '', visible\n\t\t\t\t\t[155] link 'Spaces', clickable, visible, url='https://huggingface.co/spaces'\n\t\t\t\t[157] listitem '', visible\n\t\t\t\t\t[158] link 'Posts', clickable, visible, url='https://huggingface.co/posts'\n\t\t\t\t[160] listitem '', visible\n\t\t\t\t\t[161] link 'Docs', clickable, visible, url='https://huggingface.co/docs'\n\t\t\t\t[163] listitem '', visible\n\t\t\t\t\t[164] link 'Enterprise', clickable, visible, url='https://huggingface.co/enterprise'\n\t\t\t\t[166] listitem '', visible\n\t\t\t\t\t[167] link 'Pricing', clickable, visible, url='https://huggingface.co/pricing'\n\t\t\t\t[168] listitem '', visible\n\t\t\t\t\t[170] button '', clickable, visible\n\t\t\t\t[172] listitem '', visible\n\t\t\t\t\t[173] separator '', visible, orientation='horizontal'\n\t\t\t\t[174] listitem '', visible\n\t\t\t\t\t[175] link 'Log In', clickable, visible, url='https://huggingface.co/login'\n\t\t\t\t[176] listitem '', visible\n\t\t\t\t\t[177] link 'Sign Up', clickable, visible, url='https://huggingface.co/join'\n\t[179] main ''\n\t\t[181] sectionheader '', visible\n\t\t\t[183] heading 'fixie-ai / ultravox-v0_4 Copy model name to clipboard like 35 Follow Fixie.ai 106', visible\n\t\t\t\t[188] link '', clickable, visible, url='https://huggingface.co/fixie-ai'\n\t\t\t\t\t[189] image '', visible, url='https://cdn-avatars.huggingface.co/v1/production/uploads/628d700fdb4cd1d1717c7d2f/m9n8O1Jk88UadmN6GoLNR.png'\n\t\t\t\t[192] link 'fixie-ai', clickable, visible, url='https://huggingface.co/fixie-ai'\n\t\t\t\tStaticText '/'\n\t\t\t\t[195] link 'ultravox-v0_4', clickable, visible, url='https://huggingface.co/fixie-ai/ultravox-v0_4'\n\t\t\t\t[196] button 'Copy model name to clipboard', clickable, visible\n\t\t\t\t[199] button 'like', clickable, visible\n\t\t\t\t\tStaticText 'like'\n\t\t\t\t[203] button '35', clickable, visible\n\t\t\t\t[206] button 'Follow Fixie.ai', clickable, visible\n\t\t\t\t\tStaticText 'Follow'\n\t\t\t\t\t[208] image '', visible, url='https://cdn-avatars.huggingface.co/v1/production/uploads/628d700fdb4cd1d1717c7d2f/m9n8O1Jk88UadmN6GoLNR.png'\n\t\t\t\t\tStaticText 'Fixie.ai'\n\t\t\t\t[210] button '106', clickable, visible\n\t\t\t[212] link 'Audio-Text-to-Text', clickable, visible, url='https://huggingface.co/models?pipeline_tag=audio-text-to-text'\n\t\t\t\tStaticText 'Audio-Text-to-Text'\n\t\t\t[217] link 'Transformers', clickable, visible, url='https://huggingface.co/models?library=transformers'\n\t\t\t\tStaticText 'Transformers'\n\t\t\t[221] link 'Safetensors', clickable, visible, url='https://huggingface.co/models?library=safetensors'\n\t\t\t\tStaticText 'Safetensors'\n\t\t\t[225] button '4 datasets', clickable, visible\n\t\t\t\tStaticText '4 datasets'\n\t\t\t[229] button '9 languages', clickable, visible\n\t\t\t\tStaticText '9 languages'\n\t\t\t[233] link 'ultravox', clickable, visible, url='https://huggingface.co/models?other=ultravox'\n\t\t\t\tStaticText 'ultravox'\n\t\t\t[236] link 'feature-extraction', clickable, visible, url='https://huggingface.co/models?other=feature-extraction'\n\t\t\t\tStaticText 'feature-extraction'\n\t\t\t[239] link 'custom_code', clickable, visible, url='https://huggingface.co/models?other=custom_code'\n\t\t\t\tStaticText 'custom_code'\n\t\t\t[243] button 'License: mit', clickable, visible\n\t\t\t\t[245] image '', visible\n\t\t\t\tStaticText 'License:'\n\t\t\t\tStaticText'mit'\n\t\t\t[251] link 'Model card', clickable, visible, url='https://huggingface.co/fixie-ai/ultravox-v0_4'\n\t\t\t[253] link 'Files and versions', clickable, visible, url='https://huggingface.co/fixie-ai/ultravox-v0_4/tree/main'\n\t\t\t\tStaticText 'Files and versions'\n\t\t\t[257] link 'Community 4', clickable, visible, url='https://huggingface.co/fixie-ai/ultravox-v0_4/discussions'\n\t\t\t\tStaticText '4'\n\t\t\t[263] button '', clickable, visible\n\t\t\t[267] button 'Train', clickable, visible\n\t\t\t[271] button 'Use this model', clickable, visible\n\t\t[285] button ''\n\t\t[315] heading 'Model Card for Ultravox', visible\n\t\t\t[316] link '', clickable, visible, url='https://huggingface.co/fixie-ai/ultravox-v0_4#model-card-for-ultravox'\n\t\t\tStaticText 'Model Card for Ultravox'\n\t\t[320] paragraph '', visible\n\t\t\tStaticText 'Ultravox is a multimodal Speech LLM built around a pretrained'\n\t\t\t[321] link 'Llama3.1-8B-Instruct', clickable, visible, url='https://huggingface.co/meta-llama/Meta-Llama-3.1-8B'\n\t\t\tStaticText 'and'\n\t\t\t[322] link 'Whisper-medium', clickable, visible, url='https://huggingface.co/openai/whisper-medium'\n\t\t\tStaticText 'backbone.'\n\t\t[323] paragraph '', visible\n\t\t\tStaticText 'See'\n\t\t\t[324] link 'https://ultravox.ai', clickable, visible, url='https://ultravox.ai/'\n\t\t\tStaticText 'for the GitHub repo and more information.'\n\t\t[325] heading 'Model Details', visible\n\t\t\t[326] link '', clickable, visible, url='https://huggingface.co/fixie-ai/ultravox-v0_4#model-details'\n\t\t\tStaticText 'Model Details'\n\t\t[330] heading 'Model Description', visible\n\t\t\t[331] link '', clickable, visible, url='https://huggingface.co/fixie-ai/ultravox-v0_4#model-description'\n\t\t\tStaticText 'Model Description'\n\t\t[335] paragraph '', visible\n\t\t\tStaticText 'Ultravox is a multimodal model that can consume both speech and text as input (e.g., a text system prompt and voice user message). The input to the model is given as a text prompt with a special'\n\t\t\t[336] code '', visible\n\t\t\t\tStaticText '<|audio|>'\n\t\t\tStaticText 'pseudo-token, and the model processor will replace this magic token with embeddings derived from the input audio. Using the merged embeddings as input, the model will then generate output text as usual.'\n\t\t[337] paragraph ''\n\t\t\tStaticText 'In a future revision of Ultravox, we plan to expand the token vocabulary to support generation of semantic and acoustic audio tokens, which can then be fed to a vocoder to produce voice output. No preference tuning has been applied to this revision of the model.'\n\t\t[338] list ''\n\t\t\t[339] listitem ''\n\t\t\t\tListMarker '•'\n\t\t\t\t[340] strong ''\n\t\t\t\t\tStaticText 'Developed by:'\n\t\t\t\tStaticText 'Fixie.ai'\n\t\t\t[341] listitem ''\n\t\t\t\tListMarker '•'\n\t\t\t\t[342] strong ''\n\t\t\t\t\tStaticText 'License:'\n\t\t\t\tStaticText 'MIT'\n\t\t[343] heading 'Model Sources'\n\t\t\t[344] link '', clickable, url='https://huggingface.co/fixie-ai/ultravox-v0_4#model-sources'\n\t\t\tStaticText 'Model Sources'\n\t\t[348] list ''\n\t\t\t[349] listitem ''\n\t\t\t\tListMarker '•'\n\t\t\t\t[350] strong ''\n\t\t\t\t\tStaticText 'Repository:'\n\t\t\t\tStaticText ''\n\t\t\t\t[351] link 'https://ultravox.ai', clickable, url='https://ultravox.ai/'\n\t\t\t[352] listitem ''\n\t\t\t\tListMarker '•'\n\t\t\t\t[353] strong ''\n\t\t\t\t\tStaticText 'Demo:'\n\t\t\t\tStaticText 'See repo'\n\t\t[354] heading 'Usage'\n\t\t\t[355] link '', clickable, url='https://huggingface.co/fixie-ai/ultravox-v0_4#usage'\n\t\t\tStaticText 'Usage'\n\t\t[359] paragraph ''\n\t\t\tStaticText 'Think of the model as an LLM that can also hear and understand speech. As such, it can be used as a voice agent, and also to do speech-to-speech translation, analysis of spoken audio, etc.'\n\t\t[360] paragraph ''\n\t\t\tStaticText 'To use the model, try the following:'\n\t\t[363] code ''\n\t\t\tStaticText '# pip install transformers peft librosa'\n\t\t\tStaticText ''\n\t\t\tStaticText 'import'\n\t\t\tStaticText 'transformers'\n\t\t\tStaticText 'import'\n\t\t\tStaticText 'numpy'\n\t\t\tStaticText 'as'\n\t\t\tStaticText 'np'\n\t\t\tStaticText 'import'\n\t\t\tStaticText 'librosa\n\npipe = transformers.pipeline(model='\n\t\t\tStaticText "'fixie-ai/ultravox-v0_4'"\n\t\t\tStaticText ', trust_remote_code='\n\t\t\tStaticText 'True'\n\t\t\tStaticText ')\n\npath ='\n\t\t\tStaticText '"<path-to-input-audio>"'\n\t\t\tStaticText ''\n\t\t\tStaticText '#'\n\t\t\tStaticText 'TODO:'\n\t\t\tStaticText 'pass the audio here'\n\t\t\tStaticText 'audio, sr = librosa.load(path, sr='\n\t\t\tStaticText '16000'\n\t\t\tStaticText ')\n\n\nturns = [\n  {'\n\t\t\tStaticText '"role"'\n\t\t\tStaticText ':'\n\t\t\tStaticText '"system"'\n\t\t\tStaticText ','\n\t\t\tStaticText '"content"'\n\t\t\tStaticText ':'\n\t\t\tStaticText '"You are a friendly and helpful character. You love to answer questions for people."'\n\t\t\tStaticText '},\n]\npipe({'\n\t\t\tStaticText "'audio'"\n\t\t\tStaticText ': audio,'\n\t\t\tStaticText "'turns'"\n\t\t\tStaticText ': turns,'\n\t\t\tStaticText "'sampling_rate'"\n\t\t\tStaticText ': sr}, max_new_tokens='\n\t\t\tStaticText '30'\n\t\t\tStaticText ')'\n\t\t[383] button '', clickable\n\t\t[385] heading 'Training Details'\n\t\t\t[386] link '', clickable, url='https://huggingface.co/fixie-ai/ultravox-v0_4#training-details'\n\t\t\tStaticText 'Training Details'\n\t\t[390] paragraph ''\n\t\t\tStaticText 'The model uses a pre-trained'\n\t\t\t[391] link 'Llama3.1-8B-Instruct', clickable, url='https://huggingface.co/meta-llama/Meta-Llama-3.1-8B'\n\t\t\tStaticText 'backbone as well as the encoder part of'\n\t\t\t[392] link 'Whisper-medium', clickable, url='https://huggingface.co/openai/whisper-medium'\n\t\t\tStaticText '.'\n\t\t[393] paragraph ''\n\t\t\tStaticText 'Only the multi-modal adapter is trained, while Whisper encoder and Llama are kept frozen.'\n\t\t[394] paragraph ''\n\t\t\tStaticText 'We use a knowledge-distillation loss where Ultravox is trying to match the logits of the text-based Llama backbone.'\n\t\t[395] heading 'Training Data'\n\t\t\t[396] link '', clickable, url='https://huggingface.co/fixie-ai/ultravox-v0_4#training-data'\n\t\t\tStaticText 'Training Data'\n\t\t[400] paragraph ''\n\t\t\tStaticText 'Training dataset is a mix of ASR datasets, extended by adding a "continuation" generated by Llama 3.1 8B.'\n\t\t[401] heading 'Training Procedure'\n\t\t\t[402] link '', clickable, url='https://huggingface.co/fixie-ai/ultravox-v0_4#training-procedure'\n\t\t\tStaticText 'Training Procedure'\n\t\t[406] paragraph ''\n\t\t\tStaticText 'Supervised speech to audio finetuning. For more info, see'\n\t\t\t[407] link 'training code in Ultravox repo', clickable, url='https://github.com/fixie-ai/ultravox/blob/main/ultravox/training/train.py'\n\t\t\tStaticText '.'\n\t\t[408] heading 'Training Hyperparameters'\n\t\t\t[409] link '', clickable, url='https://huggingface.co/fixie-ai/ultravox-v0_4#training-hyperparameters'\n\t\t\tStaticText 'Training Hyperparameters'\n\t\t[413] list ''\n\t\t\t[414] listitem ''\n\t\t\t\tListMarker '•'\n\t\t\t\t[415] strong ''\n\t\t\t\t\tStaticText 'Training regime:'\n\t\t\t\tStaticText 'BF16 mixed precision training'\n\t\t\t[416] listitem ''\n\t\t\t\tListMarker '•'\n\t\t\t\t[417] strong ''\n\t\t\t\t\tStaticText 'Hardward used:'\n\t\t\t\tStaticText '8x H100 GPUs'\n\t\t[418] heading 'Speeds, Sizes, Times'\n\t\t\t[419] link '', clickable, url='https://huggingface.co/fixie-ai/ultravox-v0_4#speeds-sizes-times'\n\t\t\tStaticText 'Speeds, Sizes, Times'\n\t\t"""
y = f"You are an autonomous intelligent agent that carries out a sequence of actions on a web-interface, given an instruction from a user. The actions you can take fall under the following categories:\n\nPage Operation Actions:\n`click [id]`: This action clicks on an element with a specific id on the webpage.\n`type [id] [content] [press_enter_after=0|1]`: Use this to type the content into the field with id. By default, the \"Enter\" key is pressed after typing unless press_enter_after is set to 0.\n`hover [id]`: Hover over an element with id.\n`press [key_comb]`:  Simulates the pressing of a key combination on the keyboard (e.g., Ctrl+v).\n`scroll [direction=down|up]`: Scroll the page up or down.\n\nTab Management Actions:\n`new_tab`: Open a new, empty browser tab.\n`tab_focus [tab_index]`: Switch the browser's focus to a specific tab using its index.\n`close_tab`: Close the currently active tab.\n\nURL Navigation Actions:\n`goto [url]`: Navigate to a specific URL.\n`go_back`: Navigate to the previously viewed page.\n`go_forward`: Navigate to the next page (if a previous 'go_back' action was performed).\n\nCompletion Action:\n`stop [answer]`: Issue this action when you believe the task is complete. If the objective is to find a text-based answer, provide the answer in the bracket. If you believe the task is impossible to complete, provide the answer as \"N/A\" in the bracket.\n\nYou are given the user instruction, an intermediate state of the web-page (in the form of an accessibility tree), your previous actions that got you to this web-page, and the action you took for that intermediate state. Your objective is to output your reasoning for choosing that specific action. In summary, you are given the following\nInstruction: This is the instruction given by the user.\nIntermediate State: This is the state of the web-page at some time-step t, along with all the previous actions the agent has already taken to get to this web-page.\nAction: This is the action taken by the agent at time-step t.\n\nHere are some example reasoning outputs for some random tasks\n\nInstruction:\nWhat are the temperature anomalies of climate change models?\n\nIntermediate State:\n'RootWebArea 'models - Wolfram|Alpha', focused, url='https://www.wolframalpha.com/input?i=models'\n\t[51] banner'', visible\n\t\t[52] navigation '', visible\n\t\t\t[53] list '', visible\n\t\t\t\t[54] listitem '', visible\n\t\t\t\t\t[55] button 'UPGRADE TO PRO', clickable, visible, hasPopup='menu', expanded=False\n\t\t\t\t\t\tStaticText 'UPGRADE TO PRO'\n\t\t\t\t\t\t[57] image '', visible\n\t\t\t\t[58] listitem '', visible\n\t\t\t\t\t[59] button 'APPS', clickable, visible, hasPopup='menu', expanded=False\n\t\t\t\t\t\tStaticText 'APPS'\n\t\t\t\t\t\t[61] image '', visible\n\t\t\t\t[62] listitem '', visible\n\t\t\t\t\t[63] link 'TOUR', clickable, visible, url='https://www.wolframalpha.com/tour'\n\t\t\t\t\t\tStaticText 'TOUR'\n\t\t\t\t[65] listitem '', visible\n\t\t\t\t\t[66] button 'Sign in', clickable, visible\n\t\t\t\t\t\tStaticText 'Sign in'\n\t\t\t\t[68] listitem '', visible\n\t\t\t\t\t[69] listitem '', visible\n\t\t\t\t\t\t[604] button 'Language and theme selector', clickable, visible\n\t\t\t\t\t\t\t[605] image '', visible\n\t\t\t\t\t\t\t[606] image '', visible\n\t[74] main '', visible\n\t\t[411] main '', visible\n\t\t\t[413] link 'Step by step solutions marketing image Step-by-Step Solutions with Pro Get a step ahead with your homework Go Pro Now', clickable, visible, url='https://www.wolframalpha.com/pro/pricing/students'\n\t\t\t\t[415] image 'Step by step solutions marketing image', visible, url='https://www.wolframalpha.com/_next/static/images/purpleLaptopLarge_1n5Bibtm.png'\n\t\t\t\tStaticText 'Step-by-Step Solutions with'\n\t\t\t\tStaticText 'Pro'\n\t\t\t\tStaticText 'Get a step ahead with your homework'\n\t\t\t\t[422] button 'Go Pro Now', visible\n\t\t\t\t\tStaticText 'Go'\n\t\t\t\t\tStaticText 'Pro'\n\t\t\t\t\tStaticText 'Now'\n\t\t\t\t[425] button '', clickable, visible\n\t\t\t\t\t[426] image '', visible\n\t\t\t[429] SvgRoot 'From the makers of Wolfram Language and Mathematica', visible\n\t\t\t\tgraphics-symbol ''\n\t\t\t\tgraphics-symbol ''\n\t\t\t[430] link 'WolframAlpha logo', clickable, visible, url='https://www.wolframalpha.com/'\n\t\t\t\t[431] image '', visible\n\t\t\tStaticText 'models'\n\t\t\t[439] textbox 'WolframAlpha input field' value='models', clickable, visible\n\t\t\t\tStaticText 'models'\n\t\t\t[441] button 'Compute input button', clickable, visible\n\t\t\t\t[442] image '', visible\n\t\t\t[444] list '', visible\n\t\t\t\t[445] listitem '', visible\n\t\t\t\t\t[446] button 'NATURAL LANGUAGE', clickable, visible\n\t\t\t\t\t\t[447] image '', visible\n\t\t\t\t\t\tStaticText 'NATURAL LANGUAGE'\n\t\t\t\t[449] listitem '', visible\n\t\t\t\t\t[450] button 'MATH INPUT', clickable, visible\n\t\t\t\t\t\t[451] image '', visible\n\t\t\t\t\t\tStaticText 'MATH INPUT'\n\t\t\t[454] navigation '', visible\n\t\t\t\t[455] button 'EXTENDED KEYBOARD', clickable, visible\n\t\t\t\t\t[456] image '', visible\n\t\t\t\t\tStaticText 'EXTENDED KEYBOARD'\n\t\t\t\t[458] link 'EXAMPLES', clickable, visible, url='https://www.wolframalpha.com/examples'\n\t\t\t\t\t[459] image '', visible\n\t\t\t\t\tStaticText 'EXAMPLES'\n\t\t\t\t[461] button 'UPLOAD', clickable, visible\n\t\t\t\t\t[462] image '', visible\n\t\t\t\t\tStaticText 'UPLOAD'\n\t\t\t\t[464] button 'RANDOM', clickable, visible\n\t\t\t\t\t[465] image '', visible\n\t\t\t\t\tStaticText 'RANDOM'\n\t\t\tStaticText 'Assuming'\n\t\t\tStaticText 'models'\n\t\t\tStaticText 'is occupational employment data'\n\t\t\tStaticText '|'\n\t\t\tStaticText 'Use as'\n\t\t\tbutton 'a character'\n\t\t\t\tStaticText 'a character'\n\t\t\tStaticText 'or'\n\t\t\tbutton 'a word'\n\t\t\t\tStaticText 'a word'\n\t\t\tStaticText 'instead'\n\t\t\theading 'Input interpretation'\n\t\t\timage 'models | people employed | United States'\n\t\t\tbutton 'Definitions»'\n\t\t\t\tStaticText 'Definitions'\n\t\t\t\tStaticText '»'\n\t\t\tbutton 'Download Page'\n\t\t\t\timage ''\n\t\t\t\tStaticText 'Download Page'\n\t\t\tlink 'POWERED BY THE WOLFRAM LANGUAGE', url='https://www.wolfram.com/language/index.php.en'\n\t\t\t\tStaticText 'POWERED BY THE'\n\t\t\t\tStaticText 'WOLFRAM LANGUAGE'\n\nPrevious actions:\n1.type [89] [climate models ] where [89] is WolframAlpha input field\n2.click [487] where [487] is models\n\nAction:\ntype [439] [climate models] [1]\n\nOutput: Let's think step-by-step. My objective is to find temperature anomalies of climate change models. I will type climate models into the search box, to search for a more specific model. In summary, the next action I will perform is ```type [439] [climate models] [1]```\n\nInstruction:\nFind the source code for the ultravox model\n\nIntermediate State:\n{x}\n\nPrevious actions:\n1. click [51] where [51] is\n2. click [145] where [145] is Audio-Text-to-Text.\n3. type [414] [ultravox] where [414] is Filter by name\n4. click [995] where [995] is Audio-Text-to-Text\n\nAction:\nclick [407]\n\nOutput: Let's think step-by-step. Currently, I am in the model-card section for the fixie-ai/ultravox-v0_4 model. I see a button for navigating to the git source code for this model, which I will click to navigate to the source code. The button has bid 407. In summary, the next action I will perform is ```click [407]```\n\n\n\nTo be successful, it is very important to follow the following rules:\n1. Explictly think about how executing the given action will change the web-page in a way that gets the agent closer to achieving the user instruction\n2. Make sure to wrap the action inside triple backticks (such as ```click [1234]```, ```type [12] [Hotels near CMU]```) as shown in the examples and strictly follow the format \"Output: Let's think step by step. <agent reasoning>. In summary, the next action I will perform is ```[action]``` \". Here make sure to replace [action] with the provided action."
print(y)
json_dict = {
    "intro": y,
    "examples": [],
    "template": "Instruction:\n{instruction}\n\nIntermediate State:\n{state}\n\nAction:\n{action}",
    "meta_data": {
        "observation": "accessibility_tree",
        "action_type": "id_accessibility_tree",
        "keywords": ["state", "action", "instruction"],
        "prompt_constructor": "PassivePromptConstructor",
        "answer_phrase": "Output: ",
        "action_splitter": ":",
    },
}

import json

with open("p_retroactive_reasoning.json", "w") as f:
    json.dump(json_dict, f, indent=4)
